{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56300,"status":"ok","timestamp":1658946109634,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"WoBKXQBLzulm","outputId":"f0913759-f70a-4526-ebfc-df57277e16ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting umap-learn\n","  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n","\u001b[K     |████████████████████████████████| 88 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.7.3)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n","Collecting pynndescent>=0.5\n","  Downloading pynndescent-0.5.7.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n","Building wheels for collected packages: umap-learn, pynndescent\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=ac8d54704d9cb5cdfd0f5817b2261725abb9b9a3ff5efba3fee2c2f2d09ec0dc\n","  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.7-py3-none-any.whl size=54286 sha256=8f3a0adb3c5e43a5105eb70acabc5e0937047b4d194484a8625ec481821e7326\n","  Stored in directory: /root/.cache/pip/wheels/7f/2a/f8/7bd5dcec71bd5c669f6f574db3113513696b98f3f9b51f496c\n","Successfully built umap-learn pynndescent\n","Installing collected packages: pynndescent, umap-learn\n","Successfully installed pynndescent-0.5.7 umap-learn-0.5.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting trimap\n","  Downloading trimap-1.1.4-py3-none-any.whl (15 kB)\n","Collecting annoy>=1.11\n","  Downloading annoy-1.17.0.tar.gz (646 kB)\n","\u001b[K     |████████████████████████████████| 646 kB 30.9 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from trimap) (1.0.2)\n","Requirement already satisfied: numba>=0.34 in /usr/local/lib/python3.7/dist-packages (from trimap) (0.51.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.34->trimap) (57.4.0)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba>=0.34->trimap) (1.21.6)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.34->trimap) (0.34.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->trimap) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->trimap) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->trimap) (1.7.3)\n","Building wheels for collected packages: annoy\n","  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391605 sha256=f63b936ca056aa02a67d41ea4209940e121d3bb7bf45640375d79cdcc2752960\n","  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n","Successfully built annoy\n","Installing collected packages: annoy, trimap\n","Successfully installed annoy-1.17.0 trimap-1.1.4\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n","Mounted at /content/drive\n"]}],"source":["#!pip uninstall umap\n","!pip install umap-learn\n","!pip install trimap\n","!pip install nltk\n","!pip install gensim\n","\n","%load_ext autoreload\n","%autoreload 2\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import sys\n","sys.path.insert(1, r'/content/drive/MyDrive/STU_Rafae_Baez_Ramirez/New_SNE_Code/')\n","sys.path.insert(1, r'/content/drive/MyDrive/STU_Rafae_Baez_Ramirez/New_SNE_Code/LAPS_and_GAPS-master/')\n","\n","%matplotlib inline\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import os\n","import sys\n","import math\n","import time\n","import numpy as np\n","import pandas as pd\n","import umap.umap_ as umap\n","import trimap\n","from sklearn.decomposition import PCA, KernelPCA\n","from sklearn.manifold import TSNE, Isomap, MDS\n","\n","\n","from bokeh.io import show, output_notebook, curdoc\n","from bokeh.plotting import figure\n","from bokeh.plotting import figure, output_file, show, ColumnDataSource\n","from bokeh.models import HoverTool\n","from bokeh.io import output_notebook\n","from bokeh.models.glyphs import Text\n","from bokeh.layouts import row\n","from bokeh.io import export_png\n","output_notebook()\n","\n","## Importing local Python files\n","\n","sys.path.append(r'../') \n","from src import Preprocessing, Distances, Explanation, DR_algorithms, genericMethods\n","from src.LAPS_tabular import LapsExplainer\n","from src.GAPS_tabular import GapsExplainer\n","from src.GAPS_Explanation import get_local_explanations_for_GAPS, compute_local_divergences\n","\n","def is_notebook():\n","  try:\n","    shell = get_ipython().__class__.__name__\n","    if shell == 'ZMQInteractiveShell':\n","      return True   # Jupyter notebook or qtconsole\n","    elif shell == 'TerminalInteractiveShell':\n","      return False  # Terminal running IPython\n","    else:\n","      return False  # Other type (?)\n","  except NameError:\n","    return False\n","if is_notebook():\n","    from tqdm.notebook import tqdm\n","else:\n","    from tqdm import tqdm\n","\n","Data_path = r'/content/drive/MyDrive/STU_Rafae_Baez_Ramirez/New_SNE_Code/LAPS_and_GAPS-master/data/'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":135,"status":"ok","timestamp":1658946109764,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"cLDsKVRF0TA7"},"outputs":[],"source":["def LAPS_and_plot(embedding, data_transformed, data_df, y_res, model_feats, targ, cat_feats, cat_names, num_feats, points, neighbors = 20, show_fig = False, verbose = False):\n","  exp = LapsExplainer(data_df.values, feature_names=model_feats, class_names=targ, \n","                      categorical_features=cat_feats, categorical_names=cat_names, \n","                      discretize_continuous=False, discretizer='quartile', random_state=42)\n","  laps_scores_all = []\n","  comp_scores_all = []\n","  bar = tqdm(range(len(points)))\n","\n","  for i in bar:\n","    point = points[i]\n","    start = time.time()\n","    neigh, neigh_emb, over_data, over_data_emb = exp.generate_perturbed_neighborhood(data_df.values, data_transformed, embedding, \n","                                                                                    point, data_df.values[point], nbrs=neighbors, \n","                                                                                    num_features=5, verbose = verbose)\n","    corr_feat_dist, _, _, _, _ = Explanation.explain_point_local(data_df.values[point], neigh, over_data, \n","                                                                model_feats, cat_feats, num_feats)\n","    corr_feat_dist_embd, _, _, _, _ = Explanation.explain_point_local(data_df.values[point], neigh_emb, over_data_emb, \n","                                                                      model_feats, cat_feats, num_feats)\n","    components, divergence = Explanation.compute_local_divergence(corr_feat_dist, corr_feat_dist_embd, \n","                                                                  neigh, neigh_emb)\n","    components = components.split(\",\")\n","    laps_scores_all.append(divergence)\n","    comp_scores_all.append(components)\n","  if show_fig:\n","    idx = [i for i in range(len(y_res))]\n","    colormap = {99: 'Red',\n","                -1: 'white', \n","                0: 'gold', \n","                1: 'peachpuff', \n","                2: 'indigo',\n","                3: 'green',\n","                4: 'darkolivegreen',\n","                5: 'cyan',\n","                6: 'blue',\n","                7: 'lightsteelblue',\n","                8: 'magenta',\n","                9: 'grey',\n","                98: 'limegreen'}\n","    colors = [colormap[z] for z in y_res.ravel()]\n","    labels = {99: 'POI',\n","                -1: 'Class -1', \n","                0: 'Class 0', \n","                1: 'Class 1', \n","                2: 'Class 2',\n","                3: 'Class 3',\n","                4: 'Class 4',\n","                5: 'Class 5',\n","                6: 'Class 6',\n","                7: 'Class 7',\n","                8: 'Class 8',\n","                9: 'Class 9',\n","                98: 'neighborsPOI'} \n","    annotations = [labels[z] for z in y_res.ravel()]\n","    source = ColumnDataSource(\n","          data=dict(\n","              x=embedding[:,0],\n","              y=embedding[:,1],\n","              all_colors = colors,\n","              label = annotations,\n","              indexes = idx\n","          )\n","      )\n","    hover = HoverTool(\n","          tooltips=[\n","              (\"index\", \"$index\"),\n","              (\"(x,y)\", \"($x, $y)\"),\n","          ]\n","      )\n","    curdoc().theme = 'dark_minimal'\n","    p = figure(plot_width=700, plot_height=450, tools=[hover],\n","            title=\"Embedding\")\n","    glyph = Text(x=\"x\", y=\"y\", x_offset=7, y_offset=7, text_font_size=\"9pt\", text_color=\"grey\") # text='indexes'\n","    p.circle('x', 'y', fill_color='all_colors', line_color='white', legend='label', size=10, source=source)\n","    p.add_glyph(source, glyph)\n","    print('Showing Plot')\n","    show(p)\n","  return comp_scores_all, laps_scores_all"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658946109765,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"0tFDh7A10cXe"},"outputs":[],"source":["def GAPS_score(embedding, data_transformed, data_df,  model_feats, target, cat_feats, cat_names, num_feats, POI = None, neighbors = 30, num_samples = 50, show_fig = False, verbose = False):\n","  if POI is None:\n","    rep_subset = genericMethods.generate_representative_subset(18, data_df)\n","    data_instance_numbers = rep_subset[0]\n","  else:\n","    data_instance_numbers = POI\n","\n","\n","  explainer = GapsExplainer(data_df.values, feature_names=model_feats,\n","                            class_names=target, categorical_features=cat_feats,\n","                            discretize_continuous=False, discretizer='quartile',\n","                            random_state=42)\n","\n","  neigh, neigh_emb, over_data, over_data_emb, local_feat_cont, local_feat_cont_emb, neigh_local, neigh_emb_local = explainer.generate_perturbed_neighborhood_global(\n","      data_df.values, data_transformed, embedding, data_instance_numbers, model_feats, cat_feats, num_feats, nbrs = neighbors, num_features = 5, num_samples=num_samples, verbose = verbose)\n","\n","  local_divergences = compute_local_divergences(neigh_local, neigh_emb_local, local_feat_cont, local_feat_cont_emb)\n","\n","  corr_feat_dist, _, _ = Explanation.explain_point_global(over_data, model_feats, cat_feats, num_feats, verbose=verbose)\n","  corr_feat_dist_embd, _, _ = Explanation.explain_point_global(over_data_emb, model_feats, cat_feats, num_feats, verbose=verbose)\n","\n","  components, overall_divergence = Explanation.compute_global_divergence(corr_feat_dist, corr_feat_dist_embd, neigh, neigh_emb, local_divergences)\n","  components = components.split(',')\n","  return components, overall_divergence, POI\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":146,"status":"ok","timestamp":1658946109908,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"xojN-nCT1YwE"},"outputs":[],"source":["# import nltk\n","# from nltk.tokenize import sent_tokenize, word_tokenize\n","# nltk.download('punkt')\n","# from gensim.models import Word2Vec\n","\n","# df = pd.read_csv(Data_path + 'spam.csv', encoding = \"ISO-8859-1\")\n","# labels = np.expand_dims(df['v1'].to_numpy(), -1)\n","# raw_data = df['v2'].to_numpy()\n","# data = []\n","# #print(raw_data)\n","# for sentence in raw_data:\n","#   aux = []\n","#   for word in word_tokenize(sentence):\n","#     aux.append(word.lower())\n","#   data.append(aux)\n","# dims = 100\n","# model = Word2Vec(data, min_count = 1, size = dims, window = 5)\n","# new_data = np.array([np.mean([model[w] for w in sentence if w in model] or [np.zeros(dims)], axis = 0)\n","#                      for sentence in data])\n","# new_labels = np.zeros(labels.shape)\n","# new_labels[labels == 'spam'] = 1\n","# spam_data = np.hstack([new_data, new_labels])\n","# print(spam_data.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658946109908,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"PKbXIwoj1akV"},"outputs":[],"source":["# df = pd.read_csv(Data_path + 'sentiment.csv', encoding = \"ISO-8859-1\", header = None)\n","# df.columns = ['target','id','date','flag','user','text']\n","# df = df.sample(10000)\n","# labels = np.expand_dims(df['target'].to_numpy(), -1)\n","# raw_data = df['text'].to_numpy()\n","# data = []\n","# #print(raw_data)\n","# for sentence in raw_data:\n","#   aux = []\n","#   for word in word_tokenize(sentence):\n","#     aux.append(word.lower())\n","#   data.append(aux)\n","# dims = 100\n","# model = Word2Vec(data, min_count = 1, size = dims, window = 5)\n","# new_data = np.array([np.mean([model[w] for w in sentence if w in model] or [np.zeros(dims)], axis = 0)\n","#                      for sentence in data])\n","# new_labels = np.zeros(labels.shape)\n","# new_labels[labels == 2] = 1\n","# new_labels[labels == 4] = 2\n","# sentiment_data = np.hstack([new_data, new_labels])\n","# print(sentiment_data.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":139,"status":"ok","timestamp":1658946110044,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"QB6UVHrp1isu"},"outputs":[],"source":["def load_data(data_path = Data_path, dataset_name = 'breast_cancer', amountPOI = 100, seed = 2022, max_samples = 10000):\n","  if dataset_name == 'spam':\n","    df = pd.DataFrame(spam_data)\n","  elif dataset_name == 'sentiment':\n","    df = pd.DataFrame(sentiment_data)\n","  else:\n","    df = pd.read_csv(data_path + dataset_name + '.csv')\n","  df = df.fillna(0)\n","  if df.shape[0] > max_samples:\n","    df = df.sample(max_samples)\n","  df.reset_index(inplace = True, drop = True)\n","  df.head()\n","  \n","  model_features, target = Preprocessing.set_features_and_target(df)\n","\n","  y = (df[target].values.reshape(-1, ))\n","  X_df = pd.DataFrame(df, columns=model_features)\n","\n","  np.random.seed(seed)\n","  POI = np.random.randint(0, X_df.shape[0], amountPOI)\n","\n","  X_transformed, categorical_features, numeric_features, categorical_names = Preprocessing.identify_and_transform_features(df, model_features)\n","\n","  params = [X_df, y, model_features, target, categorical_features, categorical_names, numeric_features, POI]\n","  return X_transformed, params"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658946110045,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"TGr4pe7b4cYK"},"outputs":[],"source":["def runAlgo(name = 'fSNE', data = None, max_iter = 1000, perplexity = 10, POI = [0], lambdas = 2):\n","  n_components = 2\n","  if name == 'fSNE':\n","    return fSNE_fast(data, max_iter=max_iter, POI=POI, perp=perplexity, lambdas=lambdas).fit_transform()\n","  if name == 'SNE':\n","    return fSNE_fast(data, max_iter=max_iter, POI=POI, perp=perplexity, lambdas=1).fit_transform()\n","  if name == 'UMAP':\n","    return umap.UMAP(n_neighbors=perplexity, n_components=n_components).fit_transform(X_transformed)\n","  if name == 'PCA':\n","    return PCA(n_components=n_components).fit_transform(data)\n","  if name == 'MDS':\n","    return MDS(n_components=n_components, max_iter=max_iter).fit_transform(data)\n","  if name == 'KernelPCA':\n","    return KernelPCA(n_components=n_components, max_iter=max_iter).fit_transform(data)\n","  if name == 'Isomap':\n","    return Isomap(n_components=n_components, max_iter=max_iter).fit_transform(data)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3337,"status":"ok","timestamp":1658946113378,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"3RHcXnJ30koW"},"outputs":[],"source":["from fSNE_torch import fSNE_torch as fSNE_fast\n","from fSNEparallel import fSNE as fSNE_parallel\n","from sklearn.manifold import TSNE as sklearnTSNE\n","from my_SNE import My_SNE"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":241,"status":"ok","timestamp":1658946113613,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"1jLO9pAS1ByP"},"outputs":[],"source":["iterations = 1000\n","perplexities = [10,20,30,40,50]\n","amountPOI = 100"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1658946113615,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"cS6PXFcX1Ou3"},"outputs":[],"source":["# datasets = ['breast_cancer', 'Credit_card', 'Wine_Quality', 'Magic', 'spam', 'sentiment']\n","# dr_algo = ['fSNE', 'SNE', 'UMAP', 'PCA', 'MDS', 'KernelPCA', 'Isomap']\n","# embeddings_wo_perplexities = {}\n","# datas_no_perp = {}\n","# laps_scores = []\n","# for dataset in datasets:\n","#   print(dataset)\n","#   X_transformed, params = load_data(dataset_name = dataset)\n","#   #print(X_transformed.shape)\n","#   for algo in dr_algo[3:]:\n","#     embedding = runAlgo(name=algo, data = X_transformed, max_iter = 1000, \n","#                           perplexity = 30, POI = params[7], lambdas = 2)\n","#     _, div = LAPS_and_plot(embedding, X_transformed,\n","#                             params[0], params[1],\n","#                             params[2], params[3],\n","#                             params[4], params[5],\n","#                             params[6], params[7],\n","#                             neighbors = 30, show_fig = True, verbose = False)\n","#     embeddings_wo_perplexities[f'{dataset}_{algo}'] = (embedding, div)\n","#   datas_no_perp[dataset] = X_transformed, params[0].to_numpy()\n","# np.save('embeddings_wo_perps', embeddings_wo_perplexities, allow_pickle=True)\n","# np.save('datasets_wo_perps', datas_no_perp, allow_pickle=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658946113615,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"NlwTv6v7XqiT"},"outputs":[],"source":["datasets = ['breast_cancer.csv', 'Credit_card.csv', 'Magic.csv', 'Wine_Quality.csv', 'spam', 'sentiment']\n","dr_algo = ['fSNE', 'SNE', 'UMAP', 'PCA', 'KernelPCA', 'Isomap']\n","perplexities = [10,20,30,40,50]"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1264,"status":"ok","timestamp":1658946114873,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"TLF6qy8PwaGi"},"outputs":[],"source":["datas_perp = np.load(Data_path+'spec_datasets_perp.npy', allow_pickle=True).item()\n","embeddings_w_perp = np.load(Data_path+'spec_embeddings_w_perp.npy', allow_pickle=True).item()\n","embeddings_wo_perp = np.load(Data_path+'spec_embeddings_wo_perp.npy', allow_pickle=True).item()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nna5cV_BloAz"},"outputs":[],"source":["# for i in range(len(datasets)):\n","#   (X_transformed, params) = datas_perp[datasets[i]]\n","#   for p in perplexities:\n","#     for algo in dr_algo[:3]:\n","#       key = f'{datasets[i]}_{algo.lower()}_{p}'\n","#       embedding = embeddings_w_perp[key]\n","#       if not np.isnan(embedding).any():\n","#         embedding = embeddings_w_perp[key]\n","#         _, div, _ = GAPS_score(embedding, X_transformed, \n","#                                 params[0], params[2],\n","#                                 params[3], [],\n","#                                 [], params[4],\n","#                                 params[5][:50], neighbors = 10, show_fig=False, verbose = False)\n","#         gaps = f'{np.mean(div):.5f}'\n","#         print(div)\n","#         gaps_scores[key] = gaps\n","#       else:\n","#         print(f'Input contains nan')\n","#   for algo in dr_algo[3:]:\n","#     key = f'{datasets[i]}_{algo}'\n","#     print(f'Working on {key}')\n","#     embedding = embeddings_wo_perp[key]\n","#     if not np.isnan(embedding).any():\n","#       _, div, _ = GAPS_score(embedding, X_transformed, \n","#                               params[0], params[2],\n","#                               params[3], [],\n","#                               [], params[4],\n","#                               params[5][:50], neighbors = 10, show_fig=False, verbose = False)\n","#       gaps = f'{np.mean(div):.5f}'\n","#       print(div)\n","#       gaps_scores[key] = gaps\n","#     else:\n","#       print(f'Input contains nan')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sSkQ47pgwAPe","outputId":"82aa8bd7-9053-4149-9a60-87eecf23c5e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Working on breast_cancer.csv_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02377\n","Working on breast_cancer.csv_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02255\n","Working on breast_cancer.csv_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02617\n","Working on breast_cancer.csv_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02266\n","Working on breast_cancer.csv_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02134\n","Working on breast_cancer.csv_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02476\n","Working on breast_cancer.csv_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02140\n","Working on breast_cancer.csv_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02092\n","Working on breast_cancer.csv_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02385\n","Working on breast_cancer.csv_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02103\n","Working on breast_cancer.csv_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02042\n","Working on breast_cancer.csv_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02312\n","Working on breast_cancer.csv_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02108\n","Working on breast_cancer.csv_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02073\n","Working on breast_cancer.csv_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02324\n","Working on breast_cancer.csv_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02034\n","Working on breast_cancer.csv_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02034\n","Working on breast_cancer.csv_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01993\n","Working on breast_cancer.csv_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01960\n","Working on Credit_card.csv_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02032\n","Working on Credit_card.csv_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02003\n","Working on Credit_card.csv_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02047\n","Working on Credit_card.csv_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01983\n","Working on Credit_card.csv_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01961\n","Working on Credit_card.csv_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01991\n","Working on Credit_card.csv_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01976\n","Working on Credit_card.csv_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01981\n","Working on Credit_card.csv_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01940\n","Working on Credit_card.csv_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01956\n","Working on Credit_card.csv_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01957\n","Working on Credit_card.csv_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01970\n","Working on Credit_card.csv_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01926\n","Working on Credit_card.csv_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01908\n","Working on Credit_card.csv_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01936\n","Working on Credit_card.csv_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01860\n","Working on Credit_card.csv_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01860\n","Working on Credit_card.csv_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01881\n","Working on Credit_card.csv_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01871\n","Working on Magic.csv_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01858\n","Working on Magic.csv_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01839\n","Working on Magic.csv_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01913\n","Working on Magic.csv_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01814\n","Working on Magic.csv_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01808\n","Working on Magic.csv_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01875\n","Working on Magic.csv_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01770\n","Working on Magic.csv_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01795\n","Working on Magic.csv_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01899\n","Working on Magic.csv_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01780\n","Working on Magic.csv_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01769\n","Working on Magic.csv_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01848\n","Working on Magic.csv_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01780\n","Working on Magic.csv_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01788\n","Working on Magic.csv_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01862\n","Working on Magic.csv_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01702\n","Working on Magic.csv_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01702\n","Working on Magic.csv_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01744\n","Working on Magic.csv_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01735\n","Working on Wine_Quality.csv_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01942\n","Working on Wine_Quality.csv_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01936\n","Working on Wine_Quality.csv_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02183\n","Working on Wine_Quality.csv_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01903\n","Working on Wine_Quality.csv_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01852\n","Working on Wine_Quality.csv_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02032\n","Working on Wine_Quality.csv_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01858\n","Working on Wine_Quality.csv_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01874\n","Working on Wine_Quality.csv_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02008\n","Working on Wine_Quality.csv_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01827\n","Working on Wine_Quality.csv_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01821\n","Working on Wine_Quality.csv_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01895\n","Working on Wine_Quality.csv_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01830\n","Working on Wine_Quality.csv_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01814\n","Working on Wine_Quality.csv_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01957\n","Working on Wine_Quality.csv_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01801\n","Working on Wine_Quality.csv_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01807\n","Working on Wine_Quality.csv_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01801\n","Working on Wine_Quality.csv_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.01788\n","Working on spam_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02825\n","Working on spam_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02805\n","Working on spam_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02908\n","Working on spam_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02719\n","Working on spam_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02545\n","Working on spam_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02681\n","Working on spam_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02578\n","Working on spam_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02478\n","Working on spam_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02728\n","Working on spam_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02508\n","Working on spam_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02465\n","Working on spam_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02635\n","Working on spam_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02499\n","Working on spam_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02444\n","Working on spam_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02605\n","Working on spam_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02301\n","Working on spam_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02304\n","Working on spam_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02515\n","Working on spam_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02406\n","Working on sentiment_fsne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02364\n","Working on sentiment_sne_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02279\n","Working on sentiment_umap_10\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02502\n","Working on sentiment_fsne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02305\n","Working on sentiment_sne_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02279\n","Working on sentiment_umap_20\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02388\n","Working on sentiment_fsne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02266\n","Working on sentiment_sne_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02208\n","Working on sentiment_umap_30\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02391\n","Working on sentiment_fsne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02288\n","Working on sentiment_sne_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02225\n","Working on sentiment_umap_40\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02396\n","Working on sentiment_fsne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02252\n","Working on sentiment_sne_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02154\n","Working on sentiment_umap_50\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02294\n","Working on sentiment_PCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02203\n","Working on sentiment_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n"]}],"source":["gaps_scores = {}\n","# step = 100\n","# for i in range(int(100/step)):\n","#print(gaps_scores)\n","for i in range(len(datasets)):\n","  (X_transformed, params) = datas_perp[datasets[i]]\n","  for p in perplexities:\n","    for algo in dr_algo[:3]:\n","      key = f'{datasets[i]}_{algo.lower()}_{p}'\n","      print(f'Working on {key}')\n","      embedding = embeddings_w_perp[key]\n","      if not np.isnan(embedding).any():\n","        embedding = embeddings_w_perp[key]\n","        _, div, _ = GAPS_score(embedding, X_transformed, \n","                                params[0], params[2],\n","                                params[3], [],\n","                                [], params[4],\n","                                params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","        gaps = f'{div:.5f}'\n","        print(gaps)\n","        gaps_scores[key] = gaps\n","      else:\n","        print(f'Input contains nan')\n","  for algo in dr_algo[3:]:\n","    key = f'{datasets[i]}_{algo}'\n","    print(f'Working on {key}')\n","    embedding = embeddings_wo_perp[key]\n","    if not np.isnan(embedding).any():\n","      _, div, _ = GAPS_score(embedding, X_transformed, \n","                              params[0], params[2],\n","                              params[3], [],\n","                              [], params[4],\n","                              params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","      gaps = f'{div:.5f}'\n","      print(gaps)\n","      gaps_scores[key] = gaps\n","    else:\n","      print(f'Input contains nan')\n","  key = f'{datasets[i]}_MDS'\n","  print(f'Working on {key}')\n","  embedding = MDS(n_components=2).fit_transform(X_transformed)\n","  if not np.isnan(embedding).any():\n","    _, div, _ = GAPS_score(embedding, X_transformed, \n","                            params[0], params[2],\n","                            params[3], [],\n","                            [], params[4],\n","                            params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","    gaps = f'{div:.5f}'\n","    print(gaps)\n","    gaps_scores[key] = gaps\n","  else:\n","    print(f'Input contains nan')"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":4494770,"status":"ok","timestamp":1658951300921,"user":{"displayName":"Rafael Baez Ramirez","userId":"03368220388201059112"},"user_tz":360},"id":"nu4oCyg9x4OZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31f35163-be13-4cdb-926d-0585871b3589"},"outputs":[{"output_type":"stream","name":"stdout","text":["{}\n","Working on sentiment_KernelPCA\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02203\n","Working on sentiment_Isomap\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02230\n","Working on sentiment_MDS\n","num_samples = 2\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","Step 1/4\n","Step 2/4\n","Step 3/4\n","Step 4/4\n","0.02210\n"]}],"source":["datasets = ['sentiment']\n","dr_algo = ['fSNE', 'SNE', 'UMAP', 'KernelPCA', 'Isomap']\n","perplexities = [10,20,30,40,50]\n","gaps_scores = {}\n","# step = 100\n","# for i in range(int(100/step)):\n","print(gaps_scores)\n","for i in range(len(datasets)):\n","  (X_transformed, params) = datas_perp[datasets[i]]\n","  # for p in perplexities:\n","  #   for algo in dr_algo[:3]:\n","  #     key = f'{datasets[i]}_{algo.lower()}_{p}'\n","  #     print(f'Working on {key}')\n","  #     embedding = embeddings_w_perp[key]\n","  #     if not np.isnan(embedding).any():\n","  #       embedding = embeddings_w_perp[key]\n","  #       _, div, _ = GAPS_score(embedding, X_transformed, \n","  #                               params[0], params[2],\n","  #                               params[3], [],\n","  #                               [], params[4],\n","  #                               params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","  #       gaps = f'{div:.5f}'\n","  #       print(gaps)\n","  #       gaps_scores[key] = gaps\n","  #     else:\n","  #       print(f'Input contains nan')\n","  for algo in dr_algo[3:]:\n","    key = f'{datasets[i]}_{algo}'\n","    print(f'Working on {key}')\n","    embedding = embeddings_wo_perp[key]\n","    if not np.isnan(embedding).any():\n","      _, div, _ = GAPS_score(embedding, X_transformed, \n","                              params[0], params[2],\n","                              params[3], [],\n","                              [], params[4],\n","                              params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","      gaps = f'{div:.5f}'\n","      print(gaps)\n","      gaps_scores[key] = gaps\n","    else:\n","      print(f'Input contains nan')\n","  key = f'{datasets[i]}_MDS'\n","  print(f'Working on {key}')\n","  embedding = MDS(n_components=2).fit_transform(X_transformed)\n","  if not np.isnan(embedding).any():\n","    _, div, _ = GAPS_score(embedding, X_transformed, \n","                            params[0], params[2],\n","                            params[3], [],\n","                            [], params[4],\n","                            params[5], neighbors = 5, num_samples = 2, show_fig=False, verbose = False)\n","    gaps = f'{div:.5f}'\n","    print(gaps)\n","    gaps_scores[key] = gaps\n","  else:\n","    print(f'Input contains nan')"]},{"cell_type":"code","source":[""],"metadata":{"id":"ZgrvIVN15PGx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"FinalResults-GAPS","provenance":[],"authorship_tag":"ABX9TyOI0sG/hRECq3DsXSDTLBZn"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}